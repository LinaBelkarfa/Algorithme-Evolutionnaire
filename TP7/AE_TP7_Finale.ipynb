{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithmes Évolutionnaires — Travaux dirigés N◦6 : neuroévolution\n",
    "\n",
    "Dans cette séance, nous allons développer un algorithme évolutionnaire du type NEAT pour optimiser un réseau de neurones. Pour ce faire, nous utiliserons les frameworks DEAP et Keras.\n",
    "\n",
    "### 1 Introduction\n",
    "Pour appliquer **les algorithmes évolutionnaires aux réseaux de neurones**, il y a deux possibilités : soit on code soi-même les primitives du réseau de neurones, soit on s’appuye sur un frameworks, comme **Keras**, qui fournit toutes ces primitives et plein de choses en plus. \n",
    "Dans les deux cas, il y a des avantages et des inconvénients : \n",
    "* si on décide de coder soi-même, il faudra investir du temps dans le développement, mais on aura un contrôle et une compréhension parfaite sur le code produit ; \n",
    "* si on opte pour un framework, il faudra investir du temps pour apprendre à l’utiliser, mais on aura accès à tout ce qu’il y a de plus récent et performant en matière d’apprentissage profond\n",
    "\n",
    "-------------------------------------------------------------- \n",
    "\n",
    "class = Variable de classe (1:test positif pour le diabète, 0 : test négatif pour le diabète)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NeuroEvolution of Augmenting Topologies (NEAT)\n",
    "- Démarrer avec des topologies aléatoires minimales.\n",
    "- Augmenter les topologies au fur et à mesure si nécessaire.\n",
    "- Suivez les gènes correspondants pour atténuer le problème des conventions concurrentes.\n",
    "- Protéger les innovations par la spéciation.\n",
    "\n",
    "<font color='green'> **La fonction de fitness** sera basée sur la **fonction de perte** du réseau de neurones.\n",
    "Une fonction de perte, ou **Loss function**, est une fonction qui **évalue l’écart entre les prédictions réalisées par le réseau de neurones et les valeurs réelles des observations utilisées pendant l’apprentissage**. Plus le résultat de cette fonction **est minimisé**, plus le réseau de neurones **est performant**. Sa minimisation, c’est-à-dire réduire au minimum l’écart entre la valeur prédite et la valeur réelle pour une observation donnée, se fait en ajustant les différents poids du réseau de neurones.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute the Brier score loss**\n",
    "\n",
    "Plus la perte du score de Brier est faible, mieux c'est, d'où l'appellation de \"perte\". Le score de Brier mesure la différence moyenne au carré entre la probabilité prédite et le résultat réel (mean squared difference). Le score de Brier prend toujours une valeur comprise entre zéro et un, car il s'agit de la plus grande différence possible entre une probabilité prédite (qui doit être comprise entre zéro et un) et le résultat réel (qui ne peut prendre que des valeurs comprises entre 0 et 1). Il peut être décomposé en la somme de la perte de raffinement et de la perte de calibration.\n",
    "\n",
    "Le score de Brier est approprié pour les résultats **binaires** et catégoriels qui peuvent être structurés comme **vrai ou faux**, mais il est inapproprié pour les variables ordinales qui peuvent prendre trois valeurs ou plus (ceci parce que le score de Brier suppose que tous les résultats possibles sont équivalemment \"distants\" les uns des autres). L'étiquette qui est considérée comme l'étiquette positive est contrôlée par le paramètre pos_label, qui prend par défaut l'étiquette la plus grande, sauf si y_true est tout à fait 0 ou tout à fait -1, auquel cas pos_label prend par défaut la valeur 1.\n",
    "\n",
    "RAPPEL DES CONSTANTES DE DEPART\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "* **generation** = Nombre maximum de générations = nombre max de population de réseaux de neurones testées (=> epoch = 10)\n",
    "* **population** = Taille de la population = Nombre de réseau de neurones dans la population (population = 10)\n",
    "* **mut** = Probabilité d'une mutation (<= .05)\n",
    "* **crossover** = Probabilité d'une recombination (Crossover: top 5 randomly select 2 partners)\n",
    "\n",
    "RAPPEL DES OPERATEURS \n",
    "-----------------------------------------------------------\n",
    "* Mutation : NEAT Structural Mutation \n",
    "    - Ajout de neurones\n",
    "    - Ajout de connexions\n",
    "* Recombinaison : NEAT Crossover \n",
    "    - Création d'un nouveau réseau de neurones à partir de deux réseaux parents\n",
    "* Fitness : Loss Function \n",
    "    - Minimiser la perte \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-07 10:33:41,675 Generation: 1\n",
      "\n",
      "2022-01-07 10:33:42,235 Max Fitness: 0.7441860465116279\n",
      "\n",
      "2022-01-07 10:33:42,238 optimal_weights: [array([[ 0.13603479,  0.5427541 , -0.43404847, -0.5123948 ,  0.6030039 ,\n",
      "        -0.5849414 , -0.46906245,  0.13151652],\n",
      "       [-0.08255917,  0.4236849 ,  0.56392235,  0.2022956 , -0.41092223,\n",
      "        -0.50589615, -0.05224627,  0.5120488 ],\n",
      "       [ 0.4755903 ,  0.11499125, -0.20663342, -0.46338826, -0.5820237 ,\n",
      "         0.28209662, -0.55730337,  0.19160116],\n",
      "       [ 0.5532089 , -0.42803207,  0.4314112 ,  0.2314946 ,  0.53684133,\n",
      "         0.37301975, -0.53774613,  0.0473119 ],\n",
      "       [ 0.18955189,  0.02856994,  0.4545229 ,  0.29337305,  0.49023694,\n",
      "        -0.1120407 , -0.10124892, -0.4673665 ],\n",
      "       [ 0.25819337,  0.42300683,  0.12505275,  0.3855065 ,  0.5261857 ,\n",
      "        -0.06209058,  0.3808856 , -0.1804559 ],\n",
      "       [ 0.50585765, -0.17503136,  0.56346244, -0.49886167,  0.1426146 ,\n",
      "         0.38308394, -0.42843372, -0.20645297],\n",
      "       [ 0.24484336,  0.0437054 , -0.34748113,  0.13727915, -0.5191119 ,\n",
      "         0.5200586 , -0.58167183, -0.06435984]], dtype=float32), array([[ 0.08591008],\n",
      "       [-0.5996242 ],\n",
      "       [-0.7745626 ],\n",
      "       [ 0.6344918 ],\n",
      "       [ 0.02528131],\n",
      "       [-0.35670957],\n",
      "       [-0.8012251 ],\n",
      "       [ 0.15102303]], dtype=float32)]\n",
      "\n",
      "2022-01-07 10:33:42,389 Generation: 2\n",
      "\n",
      "2022-01-07 10:33:43,340 Generation: 3\n",
      "\n",
      "2022-01-07 10:33:44,224 Generation: 4\n",
      "\n",
      "2022-01-07 10:33:44,903 Generation: 5\n",
      "\n",
      "2022-01-07 10:33:45,758 Generation: 6\n",
      "\n",
      "2022-01-07 10:33:46,438 Generation: 7\n",
      "\n",
      "2022-01-07 10:33:47,100 Generation: 8\n",
      "\n",
      "2022-01-07 10:33:48,006 Generation: 9\n",
      "\n",
      "2022-01-07 10:33:48,680 Generation: 10\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.36\n",
      "Max Fitness: 0.74\n",
      "Test Accuracy: 0.64\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG,\n",
    "                    format='%(asctime)s %(message)s',\n",
    "                    handlers=[logging.FileHandler(\"ann_test.log\"),\n",
    "                              logging.StreamHandler()])\n",
    "\n",
    "class ANN(Sequential):\n",
    "    \n",
    "    def __init__(self, child_weights=None):\n",
    "        super().__init__()\n",
    "\n",
    "        if child_weights is None:\n",
    "            layer1 = Dense(8, input_shape=(8,), activation='sigmoid')\n",
    "            layer2 = Dense(1, activation='sigmoid')\n",
    "            self.add(layer1)\n",
    "            self.add(layer2)\n",
    "        else:\n",
    "            self.add(\n",
    "                Dense(\n",
    "                    8,\n",
    "                    input_shape=(8,),\n",
    "                    activation='sigmoid',\n",
    "                    weights=[child_weights[0], np.ones(8)])\n",
    "                )\n",
    "            self.add(\n",
    "                Dense(\n",
    "                    1,\n",
    "                    activation='sigmoid',\n",
    "                    weights=[child_weights[1], np.zeros(1)])\n",
    "            )\n",
    "\n",
    "    def forward_propagation(self, train_feature, train_label):\n",
    "        predict_label = self.predict(train_feature.values)\n",
    "        # la fitness est 1/(1+loss), ainsi plus la loss est petite, plus la fitness est grande\n",
    "        self.fitness = 1/(1+ brier_score_loss(train_label, predict_label.round()))\n",
    "        #self.fitness = 1/(1 + log_loss(train_label, predict_label.round()))\n",
    "        # self.fitness = accuracy_score(train_label, predict_label.round())\n",
    "\n",
    "\n",
    "def crossover(nn1, nn2):\n",
    "    \n",
    "    nn1_weights = []\n",
    "    nn2_weights = []\n",
    "    child_weights = []\n",
    "\n",
    "    for layer in nn1.layers:\n",
    "        nn1_weights.append(layer.get_weights()[0])\n",
    "\n",
    "    for layer in nn2.layers:\n",
    "        nn2_weights.append(layer.get_weights()[0])\n",
    "\n",
    "    for i in range(len(nn1_weights)):\n",
    "        # Get single point to split the matrix in parents based on # of cols\n",
    "        split = random.randint(0, np.shape(nn1_weights[i])[1]-1)\n",
    "        # Iterate through after a single point and set the remaing cols to nn_2\n",
    "        for j in range(split, np.shape(nn1_weights[i])[1]-1):\n",
    "            nn1_weights[i][:, j] = nn2_weights[i][:, j]\n",
    "\n",
    "        child_weights.append(nn1_weights[i])\n",
    "\n",
    "    mutation(child_weights)\n",
    "\n",
    "    child = ANN(child_weights)\n",
    "    return child\n",
    "\n",
    "def mutation(child_weights):\n",
    "    selection = random.randint(0, len(child_weights)-1)\n",
    "    mut = random.uniform(0, 1)\n",
    "    if mut <= .05:\n",
    "        child_weights[selection] *= random.randint(2, 5)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "# Preprocess Data\n",
    "df = pd.read_table('./diabetes.txt',header=None,encoding='gb2312',sep='\\t')\n",
    "df.astype(float)\n",
    "# remove redundant col which is the opposite value of the 10th col\n",
    "df.pop(10)\n",
    "# remove first col of bias = 1\n",
    "df.pop(0)\n",
    "# the label column\n",
    "label = df.pop(9)\n",
    "# train feature\n",
    "train_feature = df[:576]\n",
    "# train label\n",
    "train_label = label[:576]\n",
    "# test feature\n",
    "test_feature = df[576:]\n",
    "# test label\n",
    "test_label = label[576:]\n",
    "\n",
    "# store all active ANNs\n",
    "networks = []\n",
    "pool = []\n",
    "# Generation counter\n",
    "generation = 0\n",
    "# Initial Population\n",
    "population = 10\n",
    "for i in range(population):\n",
    "    networks.append(ANN())\n",
    "# Track Max Fitness\n",
    "max_fitness = 0\n",
    "# Store Max Fitness Weights\n",
    "optimal_weights = []\n",
    "\n",
    "epochs = 10\n",
    "# Evolution Loop\n",
    "for i in range(epochs):\n",
    "    generation += 1\n",
    "    logging.debug(\"Generation: \" + str(generation) + \"\\r\\n\")\n",
    "\n",
    "    for ann in networks:\n",
    "        # Propagate to calculate fitness score\n",
    "        ann.forward_propagation(train_feature, train_label)\n",
    "        # Add to pool after calculating fitness\n",
    "        pool.append(ann)\n",
    "\n",
    "    # Clear for propagation of next children\n",
    "    networks.clear()\n",
    "\n",
    "    # Sort anns by fitness\n",
    "    pool = sorted(pool, key=lambda x: x.fitness)\n",
    "    pool.reverse()\n",
    "\n",
    "    # Find Max Fitness and Log Associated Weights\n",
    "    for i in range(len(pool)):\n",
    "        if pool[i].fitness > max_fitness:\n",
    "            max_fitness = pool[i].fitness\n",
    "\n",
    "            logging.debug(\"Max Fitness: \" + str(max_fitness) + \"\\r\\n\")\n",
    "\n",
    "            # Iterate through layers, get weights, and append to optimal\n",
    "            optimal_weights = []\n",
    "            for layer in pool[i].layers:\n",
    "                optimal_weights.append(layer.get_weights()[0])\n",
    "            logging.debug('optimal_weights: ' + str(optimal_weights)+\"\\r\\n\")\n",
    "\n",
    "    # Crossover: top 5 randomly select 2 partners\n",
    "    for i in range(5):\n",
    "        for j in range(2):\n",
    "            # Create a child and add to networks\n",
    "            temp = crossover(pool[i], random.choice(pool))\n",
    "            # Add to networks to calculate fitness score next iteration\n",
    "            networks.append(temp)\n",
    "\n",
    "# Create a Genetic Neural Network with optimal initial weights\n",
    "ann = ANN(optimal_weights)\n",
    "predict_label = ann.predict(test_feature.values)\n",
    "print('Test Loss: %.2f' % brier_score_loss(test_label, predict_label.round()))\n",
    "print('Max Fitness: %.2f' % max_fitness)\n",
    "print('Test Accuracy: %.2f' % accuracy_score(test_label, predict_label.round()))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "636cdaddf8278713ac0f8899dff8f68cf32cd8ae3d3514f61665594349bacd03"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
